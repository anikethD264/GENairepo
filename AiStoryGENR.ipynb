{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "WRd6etLy_vs9",
        "outputId": "ab2091dc-3b31-4bbb-ba1e-e699a191f058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Libraries installed successfully!\n",
            "🚀 Using device: cpu\n",
            "🎉 Creating Gradio interface...\n",
            "🚀 Launching AI Story Generator...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8455d96223d40eecc8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8455d96223d40eecc8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q transformers torch gradio accelerate sentencepiece\n",
        "\n",
        "print(\"✅ Libraries installed successfully!\")\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🚀 Using device: {device}\")\n",
        "\n",
        "class StoryGenerator:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.current_model = None\n",
        "\n",
        "    def load_gpt2_model(self):\n",
        "        \"\"\"Load GPT-2 model for story generation\"\"\"\n",
        "        print(\"📚 Loading GPT-2 model...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # Add padding token\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        self.models['gpt2'] = {\n",
        "            'tokenizer': tokenizer,\n",
        "            'model': model,\n",
        "            'pipeline': pipeline(\n",
        "                'text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "        }\n",
        "        print(\"✅ GPT-2 model loaded successfully!\")\n",
        "\n",
        "    def load_genre_story_model(self):\n",
        "        \"\"\"Load specialized genre-based story generation model\"\"\"\n",
        "        print(\"📖 Loading genre-based story model...\")\n",
        "        model_name = \"aspis/gpt2-genre-story-generation\"\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        self.models['genre'] = {\n",
        "            'tokenizer': tokenizer,\n",
        "            'model': model,\n",
        "            'pipeline': pipeline(\n",
        "                'text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "        }\n",
        "        print(\"✅ Genre-based story model loaded successfully!\")\n",
        "\n",
        "    def load_flan_t5_model(self):\n",
        "        \"\"\"Load FLAN-T5 model for story generation\"\"\"\n",
        "        print(\"🤖 Loading FLAN-T5 model...\")\n",
        "        model_name = \"google/flan-t5-base\"\n",
        "\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        self.models['flan-t5'] = {\n",
        "            'tokenizer': tokenizer,\n",
        "            'model': model,\n",
        "            'pipeline': pipeline(\n",
        "                'text2text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "        }\n",
        "        print(\"✅ FLAN-T5 model loaded successfully!\")\n",
        "\n",
        "# Initialize the story generator\n",
        "story_gen = StoryGenerator()\n",
        "\n",
        "def generate_story_gpt2(prompt, max_length=500, temperature=0.8, top_p=0.9, top_k=50):\n",
        "    \"\"\"Generate story using GPT-2 model\"\"\"\n",
        "    if 'gpt2' not in story_gen.models:\n",
        "        story_gen.load_gpt2_model()\n",
        "\n",
        "    pipeline = story_gen.models['gpt2']['pipeline']\n",
        "\n",
        "    # Generate story\n",
        "    result = pipeline(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=pipeline.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "def generate_story_genre(prompt, genre=\"adventure\", max_length=400, temperature=0.9):\n",
        "    \"\"\"Generate story using genre-specific model\"\"\"\n",
        "    if 'genre' not in story_gen.models:\n",
        "        story_gen.load_genre_story_model()\n",
        "\n",
        "    # Format prompt for genre model\n",
        "    genre_prompt = f\"<BOS> <{genre}> {prompt}\"\n",
        "\n",
        "    pipeline = story_gen.models['genre']['pipeline']\n",
        "\n",
        "    result = pipeline(\n",
        "        genre_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=pipeline.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    # Remove the genre prefix from output\n",
        "    if genre_prompt in generated_text:\n",
        "        generated_text = generated_text.replace(genre_prompt, \"\").strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def generate_story_flan_t5(prompt, max_length=400):\n",
        "    \"\"\"Generate story using FLAN-T5 model\"\"\"\n",
        "    if 'flan-t5' not in story_gen.models:\n",
        "        story_gen.load_flan_t5_model()\n",
        "\n",
        "    # Format prompt for T5\n",
        "    formatted_prompt = f\"Write a creative story based on this prompt: {prompt}\"\n",
        "\n",
        "    pipeline = story_gen.models['flan-t5']['pipeline']\n",
        "\n",
        "    result = pipeline(\n",
        "        formatted_prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=0.8,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "def generate_story_openai(prompt, model=\"gpt-3.5-turbo\", max_tokens=800):\n",
        "    \"\"\"Generate story using OpenAI API (requires API key)\"\"\"\n",
        "    try:\n",
        "        import openai\n",
        "\n",
        "        # You need to set your OpenAI API key here\n",
        "        # openai.api_key = \"your-api-key-here\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a creative story writer. Write engaging, imaginative stories based on the given prompts.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Write a creative story based on this prompt: {prompt}\"}\n",
        "            ],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0.8\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "    except ImportError:\n",
        "        return \"OpenAI library not installed. Please install with: !pip install openai\"\n",
        "    except Exception as e:\n",
        "        return f\"Error with OpenAI API: {str(e)}\"\n",
        "\n",
        "\n",
        "def generate_story(prompt, model_choice, genre=\"adventure\", max_length=500, temperature=0.8, top_p=0.9, top_k=50):\n",
        "    \"\"\"Main function to generate stories based on model choice\"\"\"\n",
        "\n",
        "    if not prompt.strip():\n",
        "        return \"⚠️ Please enter a prompt to generate a story!\"\n",
        "\n",
        "    try:\n",
        "        if model_choice == \"GPT-2 (Fast)\":\n",
        "            return generate_story_gpt2(prompt, max_length, temperature, top_p, top_k)\n",
        "\n",
        "        elif model_choice == \"Genre-Based GPT-2\":\n",
        "            return generate_story_genre(prompt, genre, max_length, temperature)\n",
        "\n",
        "        elif model_choice == \"FLAN-T5 (Creative)\":\n",
        "            return generate_story_flan_t5(prompt, max_length)\n",
        "\n",
        "        elif model_choice == \"OpenAI GPT (API Required)\":\n",
        "            return generate_story_openai(prompt)\n",
        "\n",
        "        else:\n",
        "            return \"❌ Invalid model choice!\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error generating story: {str(e)}\"\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create and launch Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"🎭 AI Story Generator\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; padding: 20px;\">\n",
        "            <h1>🎭 AI Story Generator</h1>\n",
        "            <p>Transform your single-line prompts into captivating stories!</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Input components\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"📝 Story Prompt\",\n",
        "                    placeholder=\"Enter your story idea here... (e.g., 'A detective finds a mysterious key')\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                model_choice = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"GPT-2 (Fast)\",\n",
        "                        \"Genre-Based GPT-2\",\n",
        "                        \"FLAN-T5 (Creative)\",\n",
        "                        \"OpenAI GPT (API Required)\"\n",
        "                    ],\n",
        "                    value=\"GPT-2 (Fast)\",\n",
        "                    label=\"🤖 Model Selection\"\n",
        "                )\n",
        "\n",
        "                genre_choice = gr.Dropdown(\n",
        "                    choices=[\"adventure\", \"romance\", \"mystery-&-detective\", \"fantasy\",\n",
        "                            \"humor-&-comedy\", \"paranormal\", \"science-fiction\"],\n",
        "                    value=\"adventure\",\n",
        "                    label=\"🎨 Genre (for Genre-Based model)\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                # Advanced settings\n",
        "                with gr.Accordion(\"⚙️ Advanced Settings\", open=False):\n",
        "                    max_length = gr.Slider(\n",
        "                        minimum=100, maximum=1000, value=500, step=50,\n",
        "                        label=\"📏 Max Length\"\n",
        "                    )\n",
        "                    temperature = gr.Slider(\n",
        "                        minimum=0.1, maximum=2.0, value=0.8, step=0.1,\n",
        "                        label=\"🌡️ Creativity (Temperature)\"\n",
        "                    )\n",
        "                    top_p = gr.Slider(\n",
        "                        minimum=0.1, maximum=1.0, value=0.9, step=0.1,\n",
        "                        label=\"🎯 Focus (Top-p)\"\n",
        "                    )\n",
        "                    top_k = gr.Slider(\n",
        "                        minimum=10, maximum=100, value=50, step=10,\n",
        "                        label=\"🔢 Vocabulary Size (Top-k)\"\n",
        "                    )\n",
        "\n",
        "                generate_btn = gr.Button(\"✨ Generate Story\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Output component\n",
        "                story_output = gr.Textbox(\n",
        "                    label=\"📖 Generated Story\",\n",
        "                    lines=20,\n",
        "                    max_lines=30,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "        # Example prompts\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"A robot discovers emotions for the first time\"],\n",
        "                [\"Two strangers get stuck in an elevator during a power outage\"],\n",
        "                [\"A child finds a door in their basement that leads to another world\"],\n",
        "                [\"The last bookstore on Earth refuses to close\"],\n",
        "                [\"A time traveler accidentally changes the wrong historical event\"]\n",
        "            ],\n",
        "            inputs=[prompt_input],\n",
        "            label=\"💡 Example Prompts\"\n",
        "        )\n",
        "\n",
        "        # Show/hide genre selection based on model choice\n",
        "        def update_genre_visibility(model):\n",
        "            return gr.update(visible=(model == \"Genre-Based GPT-2\"))\n",
        "\n",
        "        model_choice.change(\n",
        "            fn=update_genre_visibility,\n",
        "            inputs=[model_choice],\n",
        "            outputs=[genre_choice]\n",
        "        )\n",
        "\n",
        "        # Generate story on button click\n",
        "        generate_btn.click(\n",
        "            fn=generate_story,\n",
        "            inputs=[prompt_input, model_choice, genre_choice, max_length, temperature, top_p, top_k],\n",
        "            outputs=[story_output]\n",
        "        )\n",
        "\n",
        "        # Footer\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; padding: 20px; color: #666;\">\n",
        "            <p>🚀 Powered by Hugging Face Transformers & Gradio</p>\n",
        "            <p>💡 Tip: Try different models and settings to get varied storytelling styles!</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "    return interface\n",
        "\n",
        "\n",
        "# Create and launch the interface\n",
        "print(\"🎉 Creating Gradio interface...\")\n",
        "interface = create_gradio_interface()\n",
        "\n",
        "# Launch with public sharing enabled for Colab\n",
        "print(\"🚀 Launching AI Story Generator...\")\n",
        "interface.launch(\n",
        "    share=True,  # Creates public link for sharing\n",
        "    debug=True,  # Enable debug mode\n",
        "    server_name=\"0.0.0.0\",  # Allow external access\n",
        "    server_port=7860\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function to verify everything works\"\"\"\n",
        "    test_prompt = \"A mysterious door appears in the garden\"\n",
        "    print(f\"🧪 Testing with prompt: '{test_prompt}'\")\n",
        "\n",
        "    try:\n",
        "        story = generate_story_gpt2(test_prompt, max_length=200)\n",
        "        print(\"✅ Test successful!\")\n",
        "        print(f\"📖 Generated story preview: {story[:100]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test failed: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n🎉 Setup complete! Your AI Story Generator is ready to use!\")\n",
        "print(\"📝 Enter a prompt and watch the magic happen!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}